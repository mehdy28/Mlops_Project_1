I have this run_deployment.py : 
from pipelines.deployment_pipeline import continuous_deployment_pipeline
import click
from zenml.integrations.mlflow.model_deployers.mlflow_model_deployer import (
    MLFlowModelDeployer,
)
from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri
from zenml.integrations.mlflow.services import MLFlowDeploymentService
from typing import cast


DEPLOY = "deploy"
PREDICT = "predict"
DEPLOY_AND_PREDICT = "deploy_and_predict"


@click.command()
@click.option(
    "--config",
    "-c",
    type=click.Choice([DEPLOY, PREDICT, DEPLOY_AND_PREDICT]),
    default=DEPLOY_AND_PREDICT,
    help="Optionally you can choose to only run the deployment "
    "pipeline to train and deploy a model (`deploy`), or to "
    "only run a prediction against the deployed model "
    "(`predict`). By default both will be run "
    "(`deploy_and_predict`).",
)

@click.option(
    "--min-f1",
    default=0.92,
    help="Minimum f1 score required to deploy the model",
)
def run_deployment(config: str, min_f1: float):
    """Run the MLflow example pipeline."""
    # get the MLflow model deployer stack component
    mlflow_model_deployer_component = MLFlowModelDeployer.get_active_model_deployer()
    deploy = config == DEPLOY or config == DEPLOY_AND_PREDICT
    predict = config == PREDICT or config == DEPLOY_AND_PREDICT

    if deploy:
        # Initialize a continuous deployment pipeline run
        continuous_deployment_pipeline(
            minimum_f1 =min_f1,
            workers=3,
            timeout=60,
        )

    if predict:
        # Initialize an inference pipeline run
        """
        inference_pipeline(
            pipeline_name="continuous_deployment_pipeline",
            pipeline_step_name="mlflow_model_deployer_step",
        )
        """


    print(
        "You can run:\n "
        f"[italic green]    mlflow ui --backend-store-uri '{get_tracking_uri()}"
        "[/italic green]\n ...to inspect your experiment runs within the MLflow"
        " UI.\nYou can find your runs tracked within the "
        "`mlflow_example_pipeline` experiment. There you'll also be able to "
        "compare two or more runs.\n\n"
    )

    # fetch existing services with same pipeline name, step name and model name
    existing_services = mlflow_model_deployer_component.find_model_server(
        pipeline_name="continuous_deployment_pipeline",
        pipeline_step_name="mlflow_model_deployer_step",
        model_name="model",
    )

    if existing_services:
        service = cast(MLFlowDeploymentService, existing_services[0])
        if service.is_running:
            print(
                f"The MLflow prediction server is running locally as a daemon "
                f"process service and accepts inference requests at:\n"
                f"    {service.prediction_url}\n"
                f"To stop the service, run "
                f"[italic green]`zenml model-deployer models delete "
                f"{str(service.uuid)}`[/italic green]."
            )
        elif service.is_failed:
            print(
                f"The MLflow prediction server is in a failed state:\n"
                f" Last state: '{service.status.state.value}'\n"
                f" Last error: '{service.status.last_error}'"
            )
    else:
        print(
            "No MLflow prediction server is currently running. The deployment "
            "pipeline must run first to train a model and deploy it. Execute "
            "the same command with the `--deploy` argument to deploy a model."
        )


if __name__ == "__main__":
    run_deployment()


And this deployment_pipeline.py :
import json
import os
import numpy as np
import pandas as pd
#from materializer.custom_materializer import cs_materializer
from steps.data_ingestion import ingest_data
from steps.data_cleaning import clean_data
from steps.model_development import execute_model_tuning
from steps.model_evaluation import model_evaluation
from zenml import pipeline, step
from zenml.config import DockerSettings
from zenml.constants import DEFAULT_SERVICE_START_STOP_TIMEOUT
from zenml.integrations.constants import MLFLOW, TENSORFLOW
from zenml.integrations.mlflow.model_deployers.mlflow_model_deployer import (
    MLFlowModelDeployer,
)
from zenml.integrations.mlflow.services import MLFlowDeploymentService
from zenml.integrations.mlflow.steps import mlflow_model_deployer_step
from zenml.steps import BaseParameters, Output


docker_settings = DockerSettings(required_integrations=[MLFLOW])

class DeploymentTriggerConfig(BaseParameters):
    """Parameters that are used to trigger the deployment"""

    min_f1: float = 0.9
    
@step
def deployment_trigger(
    f1: float,
    config: DeploymentTriggerConfig,
) -> bool:
    """Implements a simple model deployment trigger that looks at the
    input model f1 and decides if it is good enough to deploy"""

    return f1 > config.min_f1



@pipeline(enable_cache=True, settings={"docker": docker_settings})
def continuous_deployment_pipeline(
    min_f1: float = 0.9,
    workers: int = 1,
    timeout: int = DEFAULT_SERVICE_START_STOP_TIMEOUT,
):
    # Link all the steps artifacts together
    df = ingest_data()
    x_train, x_test, y_train, y_test = clean_data(df)
    model = execute_model_tuning(x_train, x_test, y_train, y_test)
    accuracy ,precision , recall , f_1 = model_evaluation(model,x_test,y_test)
    deployment_decision = deployment_trigger(f1=f_1)
    mlflow_model_deployer_step(
        model=model,
        deploy_decision=deployment_decision,
        workers=workers,
        timeout=timeout,
    )



'''
@pipeline(enable_cache=False, settings={"docker": docker_settings})
def inference_pipeline(pipeline_name: str, pipeline_step_name: str):
    # Link all the steps artifacts together
    batch_data = dynamic_importer()
    model_deployment_service = prediction_service_loader(
        pipeline_name=pipeline_name,
        pipeline_step_name=pipeline_step_name,
        running=False,
    )
    predictor(service=model_deployment_service, data=batch_data)
'''

and In the steps folder I have this : 
data_cleaning.py : 
import pandas as pd
from typing import Tuple
from log import logger  # Import the logger from log.py
from model.data_cleaning import (
    DataCleaning,
    DataDivideStrategy,
    DataPreprocessStrategy,
)
from typing_extensions import Annotated
from zenml import step

@step
def clean_data(
    data: pd.DataFrame,
) -> Tuple[
    Annotated[pd.DataFrame, "x_train"],
    Annotated[pd.DataFrame, "x_test"],
    Annotated[pd.Series, "y_train"],
    Annotated[pd.Series, "y_test"],
]:
    """Data cleaning class which preprocesses the data and divides it into train and test data.

    Args:
        data: pd.DataFrame
    """
    try:
        preprocess_strategy = DataPreprocessStrategy()
        data_cleaning = DataCleaning(data, preprocess_strategy)
        preprocessed_data = data_cleaning.handle_data()
        logger.info("Data Preprocessing successful.")

        divide_strategy = DataDivideStrategy()
        data_cleaning = DataCleaning(preprocessed_data, divide_strategy)
        x_train, x_test, y_train, y_test = data_cleaning.handle_data()
        logger.info("Data dividing successful.")
        logger.info("Data cleaning successful.")

        return x_train, x_test, y_train, y_test
    except Exception as e:
        logger.error(e)
        raise e
data_ingestion.py:
import pandas as pd
from log import logger
from model.data_ingestion import IngestData
import logging
from zenml import step

@step
def ingest_data(csv_path: str) -> pd.DataFrame:
    """
    Args:
        csv_path (str): Path to the CSV file.
    Returns:
        df: pd.DataFrame
    """
    try:
        ingest_data_obj = IngestData(csv_path)
        df = ingest_data_obj.get_data()
        return df
    except Exception as e:
        logger.error(f"Error during data ingestion process: {e}")
        logging.getLogger().handlers[0].flush()
        raise e
model_development.py:
from model.model_dev import ModelTuner
from sklearn.metrics import f1_score
from log import logger
import joblib
import pandas as pd
from zenml import step
import mlflow
from zenml.client import Client
from config import model_classes, param_grid 

from sklearn.base import ClassifierMixin

experiment_tracker = Client().active_stack.experiment_tracker

@step(experiment_tracker=experiment_tracker.name)
def execute_model_tuning(
    x_train: pd.DataFrame,
    x_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
)-> ClassifierMixin:
    best_f1_score = 0
    best_model = None
    best_params = None
    best_model_name = None

    for model_name, model_class in model_classes.items():
        model = model_class()  # Create an instance of the model
        '''
        model_tuner = ModelTuner({model_name: model_class}, param_grid, x_train, y_train, x_test, y_test)
        '''
        model_tuner = ModelTuner(x_train, x_test, y_train, y_test, {model_name: model_class}, param_grid)

        logger.info(f"Training Model {model_name}")
        model_tuner.train(model_name)  # Train the model

        logger.info(f"Optimizing hyperparameters for {model_name}")
        current_best_model, current_best_params = model_tuner.optimize(model_name)

        y_pred = current_best_model.predict(x_test)
        current_f1 = f1_score(y_test, y_pred)  # Calculate F1 score

        if current_f1 > best_f1_score:
            best_f1_score = current_f1
            best_model_name = model_name
            best_params = current_best_params
            best_model = current_best_model

    print(f"Best Model - {best_model_name} - Best F1 Score: {best_f1_score}")
    print(f"Best Parameters: {best_params}")

    saved_model_filename = f"saved_model/{best_model_name}_best_model.pkl"
    joblib.dump(best_model, saved_model_filename)
    logger.info(f"Best model saved as '{saved_model_filename}'")

    return best_model
model_evaluation.py:
import pandas as pd
from model.model_evaluation import (
    Accuracy,
    Precision,
    Recall,
    F1Score
)
from zenml import step
from sklearn.base import ClassifierMixin
from typing_extensions import Annotated 
from typing import Tuple



@step
def model_evaluation(
    model:ClassifierMixin, x_test: pd.DataFrame, y_test: pd.Series
) -> Tuple[
    Annotated[float, "accuracy"],
    Annotated[float, "precision"],
    Annotated[float, "recall"],
    Annotated[float, "f_1"],
]:
    """
    Evaluate a binary classification model using specified evaluation strategies.

    Args:
        model: Binary classification model (e.g., RandomForestClassifier).
        x_test: Test features (pd.DataFrame).
        y_test: True labels (pd.Series).
        
    Returns:
        evaluation_results: A dictionary containing metric names as keys and their scores as values.
    """
    accuracy = 0
    precision = 0
    recall = 0
    f_1 = 0

    # Create instances of evaluation strategies
    accuracy_strategy = Accuracy()
    precision_strategy = Precision()
    recall_strategy = Recall()
    f1_score_strategy = F1Score()

    # Predict using the model
    y_pred = model.predict(x_test)

    # Calculate and store the evaluation metrics
    accuracy = accuracy_strategy.calculate_score(y_test.values, y_pred)
    precision = precision_strategy.calculate_score(y_test.values, y_pred)
    recall = recall_strategy.calculate_score(y_test.values, y_pred)
    f_1 = f1_score_strategy.calculate_score(y_test.values, y_pred)

    return accuracy ,precision , recall , f_1


fix the error : 