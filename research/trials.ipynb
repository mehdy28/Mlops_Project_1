{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessairy libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from typing_extensions import Annotated\n",
    "sys.path.append(os.path.abspath(os.path.join('..'))) \n",
    "from log import logger\n",
    "from config import model_classes , param_grid\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data_Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IngestData:\n",
    "    \"\"\"\n",
    "    Data ingestion class which ingests data from the source and returns a DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path) -> None:\n",
    "        \"\"\"Initialize the data ingestion class with the CSV file path.\"\"\"\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "    def get_data(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(self.csv_path)\n",
    "            logger.info(\"Data ingestion successful.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data ingestion: {e}\")\n",
    "            raise e\n",
    "\n",
    "def ingest_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file.\n",
    "    Returns:\n",
    "        df: pd.DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ingest_data_obj = IngestData(csv_path)\n",
    "        df = ingest_data_obj.get_data()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during data ingestion process: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-02 10:35:11,932: INFO: 345487086: Data ingestion successful.]\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the CSV file\n",
    "csv_path = \"../data/raw_data.csv\"\n",
    "\n",
    "# Call the ingest_data function with the CSV file path\n",
    "data_frame = ingest_data(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data_Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStrategy(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Class defining strategy for handling data\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def handle_data(self, data: pd.DataFrame) -> Union[pd.DataFrame, pd.Series]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessStrategy(DataStrategy):\n",
    "    \"\"\"\n",
    "    Data preprocessing strategy which preprocesses the data.\n",
    "    \"\"\"\n",
    "    def handle_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Removes columns which are not required, fills missing values with median average values, and converts the data type to float.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # List of columns to drop\n",
    "            columns_to_drop = ['Employee_Name', 'EmpID', 'ManagerName', 'ManagerID', 'DateofHire', 'DateofTermination','LastPerformanceReview_Date','DOB']\n",
    "            cleaned_df  = data.drop(columns=columns_to_drop, axis=1)\n",
    "            logger.info(\"Colum dropping successful.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during column dropping: {e}\")\n",
    "            raise e\n",
    "        try:\n",
    "            # Identify categorical columns\n",
    "            categorical_columns = cleaned_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "            # Create a LabelEncoder instance\n",
    "            label_encoder = LabelEncoder()\n",
    "\n",
    "            # Apply label encoding to each categorical column\n",
    "            for column in categorical_columns:\n",
    "                cleaned_df[column] = label_encoder.fit_transform(cleaned_df[column])\n",
    "            logger.info(\"Label encoding successful.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during label encoding: {e}\")\n",
    "            raise e\n",
    "        return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataDivideStrategy(DataStrategy):\n",
    "    \"\"\"\n",
    "    Data dividing strategy which divides the data into train and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    def handle_data(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Divides the data into train and test data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            X = data.drop(\"Termd\", axis=1)\n",
    "            y = data[\"Termd\"]\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42\n",
    "            )\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    Data cleaning class which preprocesses the data and divides it into train and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, strategy: DataStrategy) -> None:\n",
    "        \"\"\"Initializes the DataCleaning class with a specific strategy.\"\"\"\n",
    "        self.df = data\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def handle_data(self) -> Union[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Handle data based on the provided strategy\"\"\"\n",
    "        return self.strategy.handle_data(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(\n",
    "    data: pd.DataFrame,\n",
    ") -> Tuple[\n",
    "    Annotated[pd.DataFrame, \"x_train\"],\n",
    "    Annotated[pd.DataFrame, \"x_test\"],\n",
    "    Annotated[pd.Series, \"y_train\"],\n",
    "    Annotated[pd.Series, \"y_test\"],\n",
    "]:\n",
    "    \"\"\"Data cleaning class which preprocesses the data and divides it into train and test data.\n",
    "\n",
    "    Args:\n",
    "        data: pd.DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        preprocess_strategy = DataPreprocessStrategy()\n",
    "        data_cleaning = DataCleaning(data, preprocess_strategy)\n",
    "        preprocessed_data = data_cleaning.handle_data()\n",
    "        logger.info(\"Data Preproessing successful.\")\n",
    "\n",
    "        divide_strategy = DataDivideStrategy()\n",
    "        data_cleaning = DataCleaning(preprocessed_data, divide_strategy)\n",
    "        x_train, x_test, y_train, y_test = data_cleaning.handle_data()\n",
    "        logger.info(\"Data dividing successful.\")\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-02 10:35:13,611: INFO: 580182248: Colum dropping successful.]\n",
      "[2023-09-02 10:35:13,630: INFO: 580182248: Label encoding successful.]\n",
      "[2023-09-02 10:35:13,636: INFO: 4176965535: Data Preproessing successful.]\n",
      "[2023-09-02 10:35:13,648: INFO: 4176965535: Data dividing successful.]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test  = clean_data(data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Model_Training_&_Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all models.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def optimize(self, x_train, y_train, x_test, y_test):\n",
    "        pass\n",
    "\n",
    "class ModelTuner:\n",
    "    \"\"\"\n",
    "    Class for performing hyperparameter tuning. It uses Model strategy to perform tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_classes, param_grid, x_train, y_train, x_test, y_test):\n",
    "        self.model_classes = dict(model_classes)  # Make a copy of model_classes as a dictionary\n",
    "        self.param_grid = param_grid\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def train(self, model_name, **kwargs):\n",
    "        try:\n",
    "            model_class = self.model_classes[model_name]\n",
    "            clf = model_class(**kwargs)\n",
    "            clf.fit(self.x_train, self.y_train)\n",
    "            logger.info(f\"Model {model_name} training successful.\")\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model {model_name} training failed: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def optimize(self, model_name):\n",
    "        try:\n",
    "            model_class = self.model_classes[model_name]\n",
    "            clf = model_class()\n",
    "            grid_search = RandomizedSearchCV(clf, param_distributions=self.param_grid[model_name], n_iter=10, cv=5, n_jobs=-1)\n",
    "            grid_search.fit(self.x_train, self.y_train)\n",
    "\n",
    "            best_params = grid_search.best_params_\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            y_pred = best_model.predict(self.x_test)\n",
    "            logger.info(f\"Hyperparameter tuning for {model_name} successful.\")\n",
    "            return best_model, best_params\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hyperparameter tuning for {model_name} failed: {str(e)}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model_tuning(x_train, y_train, x_test, y_test, model_classes, param_grid):\n",
    "    best_f1_score = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_model_name = None\n",
    "\n",
    "    for model_name, model_class in model_classes.items():\n",
    "        model = model_class()  # Create an instance of the model\n",
    "        model_tuner = ModelTuner({model_name: model_class}, param_grid, x_train, y_train, x_test, y_test)\n",
    "\n",
    "        logger.info(f\"Training Model {model_name}\")\n",
    "        model_tuner.train(model_name)  # Train the model\n",
    "\n",
    "        logger.info(f\"Optimizing hyperparameters for {model_name}\")\n",
    "        current_best_model, current_best_params = model_tuner.optimize(model_name)\n",
    "\n",
    "        y_pred = current_best_model.predict(x_test)\n",
    "        current_f1 = f1_score(y_test, y_pred)  # Calculate F1 score\n",
    "\n",
    "        if current_f1 > best_f1_score:\n",
    "            best_f1_score = current_f1\n",
    "            best_model_name = model_name\n",
    "            best_params = current_best_params\n",
    "            best_model = current_best_model\n",
    "\n",
    "    print(f\"Best Model - {best_model_name} - Best F1 Score: {best_f1_score}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    saved_model_filename = f\"../saved_model/{best_model_name}_best_model.pkl\"\n",
    "    joblib.dump(best_model, saved_model_filename)\n",
    "    logger.info(f\"Best model saved as '{saved_model_filename}'\")\n",
    "\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-02 10:35:14,446: INFO: 1321146974: Training Model RandomForestClassifier]\n",
      "[2023-09-02 10:35:14,906: INFO: 3134255834: Model RandomForestClassifier training successful.]\n",
      "[2023-09-02 10:35:14,909: INFO: 1321146974: Optimizing hyperparameters for RandomForestClassifier]\n",
      "[2023-09-02 10:35:33,940: INFO: 3134255834: Hyperparameter tuning for RandomForestClassifier successful.]\n",
      "[2023-09-02 10:35:33,973: INFO: 1321146974: Training Model DecisionTreeClassifier]\n",
      "[2023-09-02 10:35:33,979: INFO: 3134255834: Model DecisionTreeClassifier training successful.]\n",
      "[2023-09-02 10:35:33,980: INFO: 1321146974: Optimizing hyperparameters for DecisionTreeClassifier]\n",
      "[2023-09-02 10:35:34,218: INFO: 3134255834: Hyperparameter tuning for DecisionTreeClassifier successful.]\n",
      "[2023-09-02 10:35:34,226: INFO: 1321146974: Training Model GradientBoostingClassifier]\n",
      "[2023-09-02 10:35:34,319: INFO: 3134255834: Model GradientBoostingClassifier training successful.]\n",
      "[2023-09-02 10:35:34,320: INFO: 1321146974: Optimizing hyperparameters for GradientBoostingClassifier]\n",
      "[2023-09-02 10:35:37,282: INFO: 3134255834: Hyperparameter tuning for GradientBoostingClassifier successful.]\n",
      "[2023-09-02 10:35:37,293: INFO: 1321146974: Training Model KNeighborsClassifier]\n",
      "[2023-09-02 10:35:37,298: INFO: 3134255834: Model KNeighborsClassifier training successful.]\n",
      "[2023-09-02 10:35:37,300: INFO: 1321146974: Optimizing hyperparameters for KNeighborsClassifier]\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[2023-09-02 10:35:37,902: INFO: 3134255834: Hyperparameter tuning for KNeighborsClassifier successful.]\n",
      "[2023-09-02 10:35:37,982: INFO: 1321146974: Best model saved as '../saved_model/RandomForestClassifier_best_model.pkl']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model - RandomForestClassifier - Best F1 Score: 1.0\n",
      "Best Parameters: {'n_estimators': 100, 'min_samples_split': 5, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "best_model = execute_model_tuning(x_train, y_train, x_test, y_test, model_classes, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Model_Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationEvaluation(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Class defining the strategy for evaluating binary classification model performance\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        pass\n",
    "\n",
    "\n",
    "class Accuracy(BinaryClassificationEvaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Accuracy for binary classification\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            accuracy: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Accuracy class\")\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            logging.info(\"The accuracy score value is: \" + str(accuracy))\n",
    "            return accuracy\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Accuracy class. Exception message: \" + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "\n",
    "class Precision(BinaryClassificationEvaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Precision for binary classification\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            precision: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Precision class\")\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            logging.info(\"The precision score value is: \" + str(precision))\n",
    "            return precision\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Precision class. Exception message: \" + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "\n",
    "class Recall(BinaryClassificationEvaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Recall for binary classification\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            recall: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Recall class\")\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            logging.info(\"The recall score value is: \" + str(recall))\n",
    "            return recall\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Recall class. Exception message: \" + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "\n",
    "class F1Score(BinaryClassificationEvaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses F1 Score for binary classification\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            f1_score: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the F1Score class\")\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            logging.info(\"The F1 score value is: \" + str(f1))\n",
    "            return f1\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the F1Score class. Exception message: \" + str(e)\n",
    "            )\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classification_evaluation(\n",
    "    model, x_test: pd.DataFrame, y_test: pd.Series\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a binary classification model using specified evaluation strategies.\n",
    "\n",
    "    Args:\n",
    "        model: Binary classification model (e.g., RandomForestClassifier).\n",
    "        x_test: Test features (pd.DataFrame).\n",
    "        y_test: True labels (pd.Series).\n",
    "        \n",
    "    Returns:\n",
    "        evaluation_results: A dictionary containing metric names as keys and their scores as values.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Create instances of evaluation strategies\n",
    "    accuracy_strategy = Accuracy()\n",
    "    precision_strategy = Precision()\n",
    "    recall_strategy = Recall()\n",
    "    f1_score_strategy = F1Score()\n",
    "\n",
    "    # Predict using the model\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate and store the evaluation metrics\n",
    "    evaluation_results[\"Accuracy\"] = accuracy_strategy.calculate_score(y_test.values, y_pred)\n",
    "    evaluation_results[\"Precision\"] = precision_strategy.calculate_score(y_test.values, y_pred)\n",
    "    evaluation_results[\"Recall\"] = recall_strategy.calculate_score(y_test.values, y_pred)\n",
    "    evaluation_results[\"F1 Score\"] = f1_score_strategy.calculate_score(y_test.values, y_pred)\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-02 10:35:38,874: INFO: 3562115558: Entered the calculate_score method of the Accuracy class]\n",
      "[2023-09-02 10:35:38,876: INFO: 3562115558: The accuracy score value is: 1.0]\n",
      "[2023-09-02 10:35:38,878: INFO: 3562115558: Entered the calculate_score method of the Precision class]\n",
      "[2023-09-02 10:35:38,885: INFO: 3562115558: The precision score value is: 1.0]\n",
      "[2023-09-02 10:35:38,888: INFO: 3562115558: Entered the calculate_score method of the Recall class]\n",
      "[2023-09-02 10:35:38,893: INFO: 3562115558: The recall score value is: 1.0]\n",
      "[2023-09-02 10:35:38,895: INFO: 3562115558: Entered the calculate_score method of the F1Score class]\n",
      "[2023-09-02 10:35:38,900: INFO: 3562115558: The F1 score value is: 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 1.0, 'Precision': 1.0, 'Recall': 1.0, 'F1 Score': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "evaluation_results = binary_classification_evaluation(best_model, x_test, y_test)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-09-02 10:35:39,398: INFO: 2467076325: Training Model RandomForestClassifier]\n",
      "[2023-09-02 10:35:39,645: INFO: 2467076325: Model training successful.]\n",
      "[2023-09-02 10:35:47,739: INFO: 2467076325: Hyperparameter tuning successful.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 102\u001b[0m\n\u001b[0;32m     99\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBest model saved as \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00msaved_model_filename\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# Log model save success\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Call the function to execute model tuning\u001b[39;00m\n\u001b[1;32m--> 102\u001b[0m execute_model_tuning(x_train, y_train, x_test, y_test)\n",
      "Cell \u001b[1;32mIn [17], line 84\u001b[0m, in \u001b[0;36mexecute_model_tuning\u001b[1;34m(x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[0;32m     82\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Model \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m current_best_model \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mtrain()  \u001b[39m# Train the model and log the training success\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m best_f1, _, current_best_params \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39moptimize()  \u001b[39m# Optimize and log the tuning success\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m best_f1 \u001b[39m>\u001b[39m best_f1_score:\n\u001b[0;32m     87\u001b[0m     best_f1_score \u001b[39m=\u001b[39m best_f1\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for all models.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, x_train, y_train):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def optimize(self, x_train, y_train, x_test, y_test):\n",
    "        pass\n",
    "\n",
    "class ModelTuner:\n",
    "    \"\"\"\n",
    "    Class for performing hyperparameter tuning. It uses Model strategy to perform tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_class, param_grid, x_train, y_train, x_test, y_test):\n",
    "        self.model_class = model_class\n",
    "        self.param_grid = param_grid\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def train(self, **kwargs):\n",
    "        try:\n",
    "            clf = self.model_class(**kwargs)\n",
    "            clf.fit(self.x_train, self.y_train)\n",
    "            logger.info(\"Model training successful.\")  # Log training success\n",
    "            return clf\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Model training failed: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    def optimize(self):\n",
    "        try:\n",
    "            clf = self.model_class()\n",
    "            grid_search = RandomizedSearchCV(clf, param_distributions=self.param_grid, n_iter=10, cv=5, n_jobs=-1)\n",
    "            grid_search.fit(self.x_train, self.y_train)\n",
    "\n",
    "            best_params = grid_search.best_params_\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            y_pred = best_model.predict(self.x_test)\n",
    "            logger.info(\"Hyperparameter tuning successful.\")  # Log tuning success\n",
    "            return  best_model, best_params\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Hyperparameter tuning failed: {str(e)}\")\n",
    "            raise e\n",
    "def execute_model_tuning(x_train, y_train, x_test, y_test):\n",
    "    # Define model classes and their respective names and hyperparameter grids\n",
    "    model_classes = [\n",
    "        (\"RandomForestClassifier\", RandomForestClassifier, {\n",
    "            'n_estimators': [10, 50, 100, 200],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }),\n",
    "        (\"DecisionTreeClassifier\", DecisionTreeClassifier, {\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "        }),\n",
    "        (\"GradientBoostingClassifier\", GradientBoostingClassifier, {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 4, 5],\n",
    "        }),\n",
    "        (\"KNeighborsClassifier\", KNeighborsClassifier, {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "        }),\n",
    "    ]\n",
    "\n",
    "    best_f1_score = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_model_name = None  # Initialize best model name variable\n",
    "\n",
    "    for model_name, model_class, param_grid in model_classes:\n",
    "        tuner = ModelTuner(model_class, param_grid, x_train, y_train, x_test, y_test)\n",
    "        logger.info(f\"Training Model {model_name}\")\n",
    "        current_best_model = tuner.train()  # Train the model and log the training success\n",
    "        best_f1, _, current_best_params = tuner.optimize()  # Optimize and log the tuning success\n",
    "\n",
    "        if best_f1 > best_f1_score:\n",
    "            best_f1_score = best_f1\n",
    "            best_model = current_best_model\n",
    "            best_params = current_best_params\n",
    "            best_model_name = model_name  # Update the best model name\n",
    "\n",
    "    # Print the best model and its parameters\n",
    "    print(f\"Best Model - {best_model_name} - Best F1 Score: {best_f1_score}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Save the best model as a .pkl file with the specified path\n",
    "    saved_model_filename = f\"../saved_model/{best_model_name}_best_model.pkl\"\n",
    "    joblib.dump(best_model, saved_model_filename)\n",
    "    logger.info(f\"Best model saved as '{saved_model_filename}'\")  # Log model save success\n",
    "\n",
    "# Call the function to execute model tuning\n",
    "execute_model_tuning(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation(ABC):\n",
    "    \"\"\"\n",
    "    Abstract Class defining the strategy for evaluating model performance in binary classification\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        pass\n",
    "\n",
    "class Accuracy(Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Accuracy\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            accuracy: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Accuracy class\")\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            logging.info(\"The accuracy value is: \" + str(accuracy))\n",
    "            return accuracy\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Accuracy class. Exception message:  \"\n",
    "                + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "class Precision(Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Precision\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            precision: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Precision class\")\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            logging.info(\"The precision value is: \" + str(precision))\n",
    "            return precision\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Precision class. Exception message:  \"\n",
    "                + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "class Recall(Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses Recall\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            recall: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the Recall class\")\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            logging.info(\"The recall value is: \" + str(recall))\n",
    "            return recall\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the Recall class. Exception message:  \"\n",
    "                + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "class F1Score(Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses F1 Score\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            f1_score: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the F1Score class\")\n",
    "            f1_score = f1_score(y_true, y_pred)\n",
    "            logging.info(\"The F1 score value is: \" + str(f1_score))\n",
    "            return f1_score\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the F1Score class. Exception message:  \"\n",
    "                + str(e)\n",
    "            )\n",
    "            raise e\n",
    "\n",
    "class RocAuc(Evaluation):\n",
    "    \"\"\"\n",
    "    Evaluation strategy that uses ROC AUC Score\n",
    "    \"\"\"\n",
    "    def calculate_score(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_true: np.ndarray\n",
    "            y_pred: np.ndarray\n",
    "        Returns:\n",
    "            roc_auc: float\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(\"Entered the calculate_score method of the RocAuc class\")\n",
    "            roc_auc = roc_auc_score(y_true, y_pred)\n",
    "            logging.info(\"The ROC AUC score value is: \" + str(roc_auc))\n",
    "            return roc_auc\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"Exception occurred in calculate_score method of the RocAuc class. Exception message:  \"\n",
    "                + str(e)\n",
    "            )\n",
    "            raise e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4b1fca65a764b45acb559e482afe389d289dd599b9f8c5fd12ff5c2ea46a65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
